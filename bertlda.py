# -*- coding: utf-8 -*-
"""bertlda_topic_modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Tmgv9ehy8OQliu24eDhQT_N391zPt8I
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
import re
import umap
import string
import time
from gensim import corpora
import gensim
from nltk.tokenize import word_tokenize
import nltk
from stop_words import get_stop_words
ntopic = 20
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')
stop_words = get_stop_words('en')
from sklearn.datasets import fetch_20newsgroups
newsgroups_train = fetch_20newsgroups(subset='train')
from pprint import pprint
num_topics = len(set(newsgroups_train.target_names))
print("num_topics : ", num_topics)
pprint(list(newsgroups_train.target_names))

# Append sentences from newsgroup documents to raw sentences
raw_sentences = []
for s in newsgroups_train.data:
    raw_sentences.append(s)

import re
"""!pip
install
umap - learn
"""

def only_letters(tested_string):
    for letter in tested_string:
        if letter not in "abcdefghijklmnopqrstuvwxyz":
            return False
    return True


# I just did an adhoc cleaning, as I see documents
# have some non English characters, so I use above method only_letters
# to filter instead of default isalpha python method
def clean_data(s):
    s = s.replace(">", "").lower()
    if "lines:" in s:
        index = s.index("lines:")
        s = s[index + 10:]

    word_list = word_tokenize(s)
    cleaned = []
    for w in word_list:
        if w not in stop_words:
            if w in string.punctuation or only_letters(w):
                if w in string.punctuation or len(set(w)) > 1:
                    cleaned.append(w)
    return " ".join(cleaned), cleaned


# from documents clean sentence and return vocublary of sentence
def build_data(docs):
    n_docs = len(docs)
    sentences = []  # sentences
    token_lists = []  # words vocublary

    for i in range(len(docs)):
        sentence, token_list = clean_data(docs[i])
        if token_list:  # if not all items eleminated
            sentences.append(sentence)
            token_lists.append(token_list)

    return sentences, token_lists


print("Number of raw sentences ", len(raw_sentences))

print("Sample raw sentence \n", raw_sentences[10])

sentences, token_lists = build_data(raw_sentences)

print(len(sentences))

print("Sentence after cleaning :\n", sentences[10])


# get tfidf of documents
def get_tfidf_embedding(items):
    tfidf = TfidfVectorizer()
    embeddings = tfidf.fit_transform(items)
    return embeddings


# Generate embedding with tfidf
embedding_tf_idf = get_tfidf_embedding(sentences)
print("Shape of sentences applied tf-idf :", embedding_tf_idf.shape)

print("Type of tf-idf vector :", type(embedding_tf_idf[10]))
print("Sample of tf-idf vector :", embedding_tf_idf[10])


# you can see tf-idf scores for row 10

def predict_topics_with_kmeans(embeddings, num_topics):
    kmeans_model = KMeans(num_topics)
    kmeans_model.fit(embeddings)
    topics_labels = kmeans_model.predict(embeddings)
    return topics_labels


def plot_embeddings(embedding, labels, title):
    labels = np.array(labels)
    distinct_labels = set(labels)

    n = len(embedding)
    counter = Counter(labels)
    for i in range(len(distinct_labels)):
        ratio = (counter[i] / n) * 100
        cluster_label = f"cluster {i}: {round(ratio, 2)}"
        x = embedding[:, 0][labels == i]
        y = embedding[:, 1][labels == i]
        plt.plot(x, y, '.', alpha=0.4, label=cluster_label)
    plt.legend(title="Topic", loc='upper left', bbox_to_anchor=(1.01, 1))
    plt.title(title)


def reduce_umap(embedding):
    reducer = umap.UMAP()  # umap.UMAP()
    embedding_umap = reducer.fit_transform(embedding)
    return embedding_umap


def reduce_pca(embedding):
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(embedding)
    print("pca explained_variance_ ", pca.explained_variance_)
    print("pca explained_variance_ratio_ ", pca.explained_variance_ratio_)

    return reduced


def reduce_tsne(embedding):
    tsne = TSNE(n_components=2)
    reduced = tsne.fit_transform(embedding)

    return reduced


# Apply kmeans to raw vectors
labels_tfidf_raw = predict_topics_with_kmeans(embedding_tf_idf, num_topics)

print("Embedding Tf-idf shape :", embedding_tf_idf.shape)

# Apply kmeans to umap vectors
embedding_tf_idf_umap = reduce_umap(embedding_tf_idf)
labels_tfidf_umap = predict_topics_with_kmeans(embedding_tf_idf_umap, num_topics)

print("Embedding shape after umap", embedding_tf_idf_umap.shape)

plot_embeddings(embedding_tf_idf_umap, labels_tfidf_umap, "Tf-idf with Umap")

embedding_tf_idf_tsne = reduce_tsne(embedding_tf_idf)
labels_tfidf_tsne = predict_topics_with_kmeans(embedding_tf_idf_tsne, num_topics)

plot_embeddings(embedding_tf_idf_tsne, labels_tfidf_tsne, "Tf-idf with T-sne")

# The silhouette value is a measure of how similar an object is to its own cluster compared to other clusters

# The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.
# Negative values generally indicate that a sample has
# been assigned to the wrong cluster, as a different cluster is more similar.


print("Silhouette score:")
print("without dim reduction :", silhouette_score(embedding_tf_idf, labels_tfidf_raw))
print("with Tf-idf   Umap    :", silhouette_score(embedding_tf_idf_umap, labels_tfidf_umap))
print("with Tf-idf   T-sne   :", silhouette_score(embedding_tf_idf_tsne, labels_tfidf_tsne))


def get_document_topic_lda(model, corpus, k):
    n_doc = len(corpus)
    # init a vector of size number of docs x clusters
    document_topic_mapping = np.zeros((n_doc, k))
    for i in range(n_doc):
        # for each document create a vector of probability document belonging to topic
        for topic, prob in model.get_document_topics(corpus[i]):
            document_topic_mapping[i, topic] = prob

    return document_topic_mapping


print("Number of words in token list :", len(token_lists))

dictionary = corpora.Dictionary(token_lists)
corpus = [dictionary.doc2bow(text) for text in token_lists]
k = ntopic
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word=dictionary, passes=20)

embedding_lda = get_document_topic_lda(ldamodel, corpus, k)

# This is not a document embedding , I just use it as it is.
print("LDA vector shape :", embedding_lda.shape)

for i, topic in enumerate(embedding_lda[10].flatten()):
    print("Topic ", i + 1, ") ", embedding_lda[10].flatten()[i])

print("Number of tokens : ", len(token_lists))
print("Sample item from corpus length :", len(corpus[100]))
print("Sample item from corpus vector :", corpus[100])

print("Token 0 : ", dictionary.id2token[0])

list(token_lists[100]).count(dictionary.id2token[10])

ldamodel.get_document_topics(corpus[0])

srtd = sorted(ldamodel.get_document_topics(corpus[0]), key=lambda x: x[1], reverse=True)
print(srtd)
print(srtd[0][0])

labels_lda = []
for line in corpus:
    line_labels = sorted(ldamodel.get_document_topics(line), key=lambda x: x[1], reverse=True)
    # 1st 0 is for selecting top item, and 2nd 0 is for index of tuple
    top_topic = line_labels[0][0]
    labels_lda.append(top_topic)

np.array(labels_lda).shape

# Since LDA already has a low dimension num_topic ( 20 ) , dimension reductions
# will not yield good results


embedding_umap_lda = reduce_umap(embedding_lda)

plot_embeddings(embedding_umap_lda, labels_lda, "LDA with Umap")

embedding_pca_lda = reduce_pca(embedding_lda)
plot_embeddings(embedding_pca_lda, labels_lda, "LDA with PCA")

embedding_tsne_lda = reduce_tsne(embedding_lda)
plot_embeddings(embedding_tsne_lda, labels_lda, "LDA with T-sne")

print("Silhouette score:")
print("LDA          : ", silhouette_score(embedding_lda, labels_lda))

print("LDA with PCA : ", silhouette_score(embedding_pca_lda, labels_lda))

print("LDA with TSNE : ", silhouette_score(embedding_tsne_lda, labels_lda))

print("LDA with UMAP : ", silhouette_score(embedding_umap_lda, labels_lda))
"""
!pip
install - U
sentence - transformers
"""
from sentence_transformers import SentenceTransformer

model_bert = SentenceTransformer('bert-base-nli-max-tokens')

embedding_bert = np.array(model_bert.encode(sentences, show_progress_bar=True))

# Bert embeddings are shape of 768
print("Bert Embedding shape", embedding_bert.shape)
print("Bert Embedding sample", embedding_bert[0][0:50])

# Apply Kmeans without dimension reduction
labels_bert_raw = predict_topics_with_kmeans(embedding_bert, num_topics)

# Apply Kmeans for Bert Vectors  with Umap  dimension reduction

embedding_umap_bert = reduce_umap(embedding_bert)
labels_bert_umap = predict_topics_with_kmeans(embedding_umap_bert, num_topics)
plot_embeddings(embedding_umap_bert, labels_bert_umap, "Bert with Umap")

# Apply Kmeans for Bert Vectors  with PCA  dimension reduction

embedding_bert_pca = reduce_pca(embedding_bert)
labels_bert_pca = predict_topics_with_kmeans(embedding_bert_pca, num_topics)

plot_embeddings(embedding_bert_pca, labels_bert_pca, "Bert with PCA")

# Apply Kmeans for Bert Vectors  with T-sne  dimension reduction


embedding_bert_tsne = reduce_tsne(embedding_bert)
labels_bert_tsne = predict_topics_with_kmeans(embedding_bert_tsne, num_topics)
plot_embeddings(embedding_bert_tsne, labels_bert_tsne, "Bert with T-sne")

print("Silhouette score:")

print("Raw Bert", silhouette_score(embedding_bert, labels_bert_raw))

print("Bert with PCA", silhouette_score(embedding_bert_pca, labels_bert_pca))

print("Bert with Tsne", silhouette_score(embedding_bert_tsne, labels_bert_tsne))

print("Bert with Umap", silhouette_score(embedding_umap_bert, labels_bert_umap))
